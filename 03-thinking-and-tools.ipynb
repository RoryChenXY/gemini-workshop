{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrD2kWanydfP"
      },
      "source": [
        "##### Copyright 2025 Patrick Loeber, Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wrgUJetgydfR"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si1uWsxtj0W6"
      },
      "source": [
        "# Workshop: Build with Gemini (Part 3)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickloeber/workshop-build-with-gemini/blob/cloud-summit-nordics/03-thinking-and-tools.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This workshop teaches how to build with Gemini using the Gemini API and Python SDK.\n",
        "\n",
        "Course outline:\n",
        "\n",
        "- **[Part1: Quickstart + Text prompting](https://github.com/patrickloeber/workshop-build-with-gemini/blobcloud-summit-nordics/01-text-prompting.ipynb)**\n",
        "\n",
        "- **[Part 2: Multimodal understanding (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/cloud-summit-nordics/02-multimodal-understanding.ipynb)**\n",
        "\n",
        "- **Part 3 (this notebook): Thinking models + agentic capabilities (tool usage)**\n",
        "  - Thinking models\n",
        "  - Structured outputps\n",
        "  - Function calling\n",
        "  - Code execution\n",
        "  - Grounding with Google Search\n",
        "  - URL Context\n",
        "  - Final excercise: Give Gemini access to the Pok√©API to answer Pok√©mon questions\n",
        "\n",
        "## 0. Use the Google AI Studio as playground\n",
        "\n",
        "Explore and play with all models in the [Google AI Studio](https://aistudio.google.com/apikey).\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "Get a free API key in the [Google AI Studio](https://aistudio.google.com/apikey) and set up the [Google Gen AI Python SDK](https://github.com/googleapis/python-genai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SzjZdf7mwD_",
        "outputId": "21c88fc8-2f89-40c0-8d82-0e4f2769d2c7"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF3gXZyFm3Pf"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "else:\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "MODEL = \"gemini-2.5-flash-preview-05-20\" # \"gemini-2.5-pro-preview-06-05\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drS_GiIih9kw"
      },
      "source": [
        "## Thinking models\n",
        "\n",
        "Starting with Gemini 2.5, all models have thinking capabilities. These models use an internal \"thinking process\" during response generation. This process contributes to their improved reasoning capabilities and allows them to solve complex tasks, particularly complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents.\n",
        "\n",
        "Thinking models are also great at working with tools to perform actions beyond generating text. This allows them to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.\n",
        "\n",
        "(Note: Tools are also available with Gemini 2.0 models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iRjV4L-sMxp",
        "outputId": "e8d17a05-7371-4e89-996f-a89a6c6367bd"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"If it takes 5 minutes to boil one egg, how long does it take to boil three eggs?\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thinking budgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Provide a list of 3 famous physicists and their key contributions\",\n",
        "    config={\"thinking_config\": {\"thinking_budget\": 1024}}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "  model=MODEL,\n",
        "  contents=\"What is the sum of the first 10 prime numbers?\",\n",
        "  config={\"thinking_config\": {\"include_thoughts\": True}}\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.thought:\n",
        "        print(\"Thought summary:\")\n",
        "        print(part.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-qkPEabTURX"
      },
      "source": [
        "## Structured output\n",
        "\n",
        "Gemini generates unstructured text by default, but some applications require structured text. For these use cases, you can constrain Gemini to respond with JSON, a structured data format suitable for automated processing. You can also constrain the model to respond with one of the options specified in an enum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsnYkEF2Tcm8"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    recipe_name: str\n",
        "    ingredients: list[str]\n",
        "    prep_time_minutes: int\n",
        "\n",
        "class RecipeList(BaseModel):\n",
        "    recipes: list[Recipe]\n",
        "\n",
        "# Using Pydantic models for structured output\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Give me 2 popular cookie recipes with ingredients and prep details.\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=RecipeList,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Get structured data directly\n",
        "recipes: list[Recipe] = response.parsed\n",
        "for recipe in recipes.recipes:\n",
        "    print(f\"Recipe: {recipe.recipe_name}\")\n",
        "    print(f\"Ingredients: {recipe.ingredients}\")\n",
        "    print(f\"Prep Time: {recipe.prep_time_minutes} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKRcuZE_Rjl-"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL1FX3euRlQN"
      },
      "outputs": [],
      "source": [
        "def get_weather(location: str) -> dict:\n",
        "    \"\"\"Gets current weather for a location.\n",
        "    \n",
        "    Args:\n",
        "        location: The city name, e.g. \"San Francisco\"\n",
        "        \n",
        "    Returns:\n",
        "        Weather information dictionary\n",
        "    \"\"\"\n",
        "    # Mock weather data - in real use, you'd call a weather API\n",
        "    weather_data = {\n",
        "        \"temperature\": 22,\n",
        "        \"condition\": \"sunny\", \n",
        "        \"humidity\": 60,\n",
        "        \"location\": location,\n",
        "        \"feels_like\": 24\n",
        "    }\n",
        "    print(f\"üå§Ô∏è FUNCTION CALLED: get_weather(location='{location}')\")\n",
        "    return weather_data\n",
        "\n",
        "# Define function declarations for the model\n",
        "weather_function = {\n",
        "    \"name\": \"get_weather\",\n",
        "    \"description\": \"Gets current weather for a location\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city name\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"location\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.genai.types import Tool\n",
        "\n",
        "function_tool = Tool(function_declarations=[weather_function])\n",
        "\n",
        "# Define user prompt\n",
        "contents = [\n",
        "    types.Content(\n",
        "        role=\"user\", parts=[types.Part(text=\"Whats the weather in Tokyo?\")]\n",
        "    )\n",
        "]\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=contents,\n",
        "    config={\"tools\":[function_tool]}\n",
        ")\n",
        "\n",
        "# Check for function calls\n",
        "function_call = response.candidates[0].content.parts[0].function_call\n",
        "print(f\"Model wants to call: {function_call.name}\")\n",
        "print(f\"With arguments: {dict(function_call.args)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the function\n",
        "if function_call.name == \"get_weather\":\n",
        "    result = get_weather(**function_call.args)\n",
        "else:\n",
        "    result = {\"error\": \"Unknown function\"}\n",
        "\n",
        "print(f\"Function result: {result}\")\n",
        "\n",
        "# Send function result back to model\n",
        "function_response_part = types.Part.from_function_response(\n",
        "    name=function_call.name,\n",
        "    response={\"result\": result}\n",
        ")\n",
        "# Append function call and result of the function execution to contents\n",
        "contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=function_call)])) # Append the model's function call message\n",
        "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
        "\n",
        "# Get final response\n",
        "final_response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=contents,\n",
        "    config={\"tools\":[function_tool]}\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal response: {final_response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpV5sW9B0oBg"
      },
      "source": [
        "### Automatic Function Calling (Python Only)\n",
        "\n",
        "When using the Python SDK, you can provide Python functions directly as tools.\n",
        "\n",
        "The SDK handles the function call and returns the final text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqL-113f020c",
        "outputId": "bc0d8c7c-8272-404b-ab34-9bc8ad1f712d"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What's the temperature in Boston?\",\n",
        "    config={\n",
        "        \"tools\":[get_weather],\n",
        "        # to diable automatic funtion calling, you can set this:\n",
        "        # \"automatic_function_calling\": { \"disable\": True }\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okP9B1cJ7yKi"
      },
      "source": [
        "Check the function calling history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQwksiK7z1X",
        "outputId": "1df3edab-a3e8-4248-f5de-8807ae2e124f"
      },
      "outputs": [],
      "source": [
        "for content in response.automatic_function_calling_history:\n",
        "    for part in content.parts:\n",
        "        if part.function_call:\n",
        "            print(part.function_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16C3AP4YOVlc"
      },
      "source": [
        "## Code execution\n",
        "\n",
        "The code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUMFt9wqOgVk"
      },
      "outputs": [],
      "source": [
        "from google.genai.types import Tool, ToolCodeExecution\n",
        "\n",
        "code_tool = Tool(code_execution=ToolCodeExecution)\n",
        "\n",
        "# In your prompt, give instruction to use/generate code\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL,\n",
        "  contents='Create a bar chart showing the population of the 5 largest cities in the world. Generate code to solve it. Use matplotlib.',\n",
        "  config={\"tools\":[code_tool]}\n",
        ")\n",
        "\n",
        "\n",
        "from IPython.display import Image, Markdown, HTML\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.text is not None:\n",
        "        display(Markdown(part.text))\n",
        "    if part.executable_code is not None:\n",
        "        display(Markdown(f\"```python\\n{part.executable_code.code}\\n```\"))\n",
        "    if part.code_execution_result is not None:\n",
        "        display(Markdown(f\"#### Output\\n{part.code_execution_result.output}\"))\n",
        "    if part.inline_data is not None:\n",
        "        display(Image(data=part.inline_data.data, width=800, format=\"png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0m5rasbQsDa"
      },
      "source": [
        "## Grounding with Google Search\n",
        "\n",
        "If Google Search is configured as a tool, Gemini can decide when to use Google Search to improve the accuracy and recency of responses.\n",
        "\n",
        "Here's a question about a recent event without Google Search:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "582GKc2DQ-N6",
        "outputId": "49dcb0f1-703a-4dc6-ac4d-b6f06d0b21cf"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Who won the super bowl in 2025?\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SREuxqDSQs1y"
      },
      "outputs": [],
      "source": [
        "from google.genai.types import Tool, GoogleSearch\n",
        "\n",
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Who won the super bowl in 2025?\",\n",
        "    config={\"tools\":[google_search_tool]}\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "BUgF_qSFQ6KW",
        "outputId": "0cfc43da-0b07-47ce-e46e-9e21839aad8c"
      },
      "outputs": [],
      "source": [
        "# To get grounding metadata as web content.\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## URL Context Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.genai.types import Tool, UrlContext\n",
        "\n",
        "url_context_tool = Tool(url_context=UrlContext())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Summarize the key features and benefits mentioned on https://www.python.org/about/ in 3 bullet points.\",\n",
        "    config={\"tools\":[url_context_tool]}\n",
        ")\n",
        "\n",
        "print(\"üåê Python.org Summary:\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfnxbheQ5GsO"
      },
      "source": [
        "## !! Exercise !!: Get Pok√©mon stats\n",
        "\n",
        "- Define a function that can work with the Pok√©API and get Pok√©mon stats.\n",
        "- Endpoint to use: `GET https://pokeapi.co/api/v2/pokemon/<pokekon_name>`\n",
        "- Call Gemini and give it access to the function, then answer questions like: `\"What stats does the Pokemon Squirtle have?\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNe1p_BkVOmu",
        "outputId": "b997e60e-213a-4315-f1e9-c10e00c33607"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_pokemon_info(pokemon: str) -> dict:\n",
        "    \"\"\"Gets pokemon info for a given pokemon name.\n",
        "\n",
        "    Args:\n",
        "        pokemon: The name of the pokemon.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the info.\n",
        "    \"\"\"\n",
        "    url = f\"https://pokeapi.co/api/v2/pokemon/{pokemon.lower()}\"\n",
        "    # TODO: send GET request to endpoint and return JSON\n",
        "\n",
        "\n",
        "# Configure the function call\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"What stats does the Pokemon Squirtle have?\",\n",
        "    config=...\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg1Bu_VgdARK",
        "outputId": "4ce2c6f6-3425-445d-f7ae-3d4c8d5b59c4"
      },
      "outputs": [],
      "source": [
        "for content in response.automatic_function_calling_history:\n",
        "    for part in content.parts:\n",
        "        if part.function_call:\n",
        "            print(part.function_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTWLa3ZZ_yhT"
      },
      "source": [
        "## Recap & Next steps\n",
        "\n",
        "Awesome work! You learned about thinking models with advanced reasoning capabilities and how to combine Gemini with tools for agentic use cases.\n",
        "\n",
        "More helpful resources:\n",
        "\n",
        "- [Thinking docs](https://ai.google.dev/gemini-api/docs/thinking)\n",
        "- [Structured output docs](https://ai.google.dev/gemini-api/docs/structured-output?lang=python)\n",
        "- [Code execution docs](https://ai.google.dev/gemini-api/docs/code-execution?lang=python)\n",
        "- [Grounding docs](https://ai.google.dev/gemini-api/docs/grounding?lang=python)\n",
        "- [Function calling docs](https://ai.google.dev/gemini-api/docs/function-calling?example=weather)\n",
        "- [URL context docs](https://ai.google.dev/gemini-api/docs/url-context)\n",
        "\n",
        "üéâüéâ**Conratulations, you completed the workshop!**üéâüéâ\n",
        "\n",
        "**Next steps**: There's even more you can do with Gemini which we didn't cover in this workshop:\n",
        "\n",
        "- **[Part 4: Try the Live API](https://github.com/patrickloeber/workshop-build-with-gemini/blob/cloud-summit-nordics/04-live-api)**\n",
        "- **[Part 5: Use MCP with Gemini](https://github.com/patrickloeber/workshop-build-with-gemini/blob/cloud-summit-nordics/05-mcp)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
