{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuC_VSKMcEt6"
      },
      "source": [
        "# Workshop: Build with Gemini (Part 1)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickloeber/workshop-build-with-gemini/blob/main/notebooks/part-1-text-prompting.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This workshop teaches how to build with Gemini using the Gemini API and Python SDK.\n",
        "\n",
        "Course outline:\n",
        "\n",
        "- **Part1 (this notebook): Quickstart + Text prompting**\n",
        "  - Text understanding\n",
        "  - Streaming response\n",
        "  - Chats\n",
        "  - System prompts\n",
        "  - Config options\n",
        "  - Long context\n",
        "  - Token usage\n",
        "  - Final excercise: Chat with book\n",
        "\n",
        "- **[Part 2: Multimodal understanding (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/cookbooks/part-2-multimodal-understanding.ipynb)**\n",
        "\n",
        "- **[Part 3: Thinking models + agentic capabilities (tool usage)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/cookbooks/part-3-thinking-and-tools.ipynb)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avRVsnMMJvof"
      },
      "source": [
        "## 0. Use the Google AI Studio as playground\n",
        "\n",
        "Explore and play with all models in the [Google AI Studio](https://aistudio.google.com/apikey).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnl6q8tMcpwU"
      },
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD1kaBP4dnZG"
      },
      "source": [
        "Get a free API key in the [Google AI Studio](https://aistudio.google.com/apikey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6raUs82eYfk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKjUEGGzdp87"
      },
      "source": [
        "Install the [Google Gen AI Python SDK](https://github.com/googleapis/python-genai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4d9NjqNeAXx"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b7d1FleDuz"
      },
      "source": [
        "Configure Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6Uort3heUqT"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2KmoPSgRxO"
      },
      "source": [
        "Configure model. See all [models](https://ai.google.dev/gemini-api/docs/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0qcgiiP7gO-6"
      },
      "outputs": [],
      "source": [
        "MODEL = ... # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLsGbeGec8iF"
      },
      "source": [
        "## 2. Send your first prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57RFdZ6dRro"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqnTYJFdSlG"
      },
      "source": [
        "## 3. Text understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHRVaK0-tCE_"
      },
      "source": [
        "The simplest way to generate text is to provide the model with a text-only prompt. `contents` can be a single prompt, a list of prompts, or a combination of multimodal inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_HqjSiFsUQ2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itCzXz1BiG5g"
      },
      "source": [
        "#### Streaming response\n",
        "\n",
        "By default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return outputs as they're generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d6HzwfZdWbt"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZjfCkzSdcEc"
      },
      "source": [
        "#### Chat\n",
        "\n",
        "The SDK chat class provides an interface to keep track of conversation history. Behind the scenes it uses the same `generate_content` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCI8O9Ldjn6q"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmfMuI44Kev2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_MkOG6uLs75"
      },
      "source": [
        "#### Parameters\n",
        "\n",
        "Every prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_jk93Z-Lum-"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPyrJ9ul7yuv"
      },
      "source": [
        "- `max_output_tokens`: Sets the maximum number of tokens to include in a candidate.\n",
        "- `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n",
        "- `top_p`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the top_p value.\n",
        "- `top_k`: Changes how the model selects tokens for output. A top_k of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a top_k of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on top_p with the final token selected using temperature sampling.\n",
        "- `stop_sequences`: List of strings  (up to 5) that tells the model to stop generating text if one of the strings is encountered in the response. If specified, the API will stop at the first appearance of a stop sequence.\n",
        "- `seed`: If specified, the model makes a best effort to provide the same response for repeated requests. By default, a random number is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9JgfKF8nvr"
      },
      "source": [
        "#### System instructions\n",
        "\n",
        "System instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CayVOonC8st5"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjdRzLbN-ANo"
      },
      "source": [
        "#### Long context and token counting\n",
        "\n",
        "Gemini 2.0 Flash and 2.5 Pro have a 1M token context window.\n",
        "\n",
        "In practice, 1 million tokens could look like:\n",
        "\n",
        "- 50,000 lines of code (with the standard 80 characters per line)\n",
        "- All the text messages you have sent in the last 5 years\n",
        "- 8 average length English novels\n",
        "- 1 hour of video data\n",
        "\n",
        "Let's feed in an entire book and ask questions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6pGhOkj-CFS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "res = requests.get(\"https://gutenberg.org/cache/epub/16317/pg16317.txt\")\n",
        "book = res.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0nnKaKC-NMu"
      },
      "outputs": [],
      "source": [
        "print(book[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ves9N2m-_k-V"
      },
      "outputs": [],
      "source": [
        "print(f\"# charakters {len(book)}\")\n",
        "print(f\"# words {len(book.split())}\")\n",
        "print(f\"# tokens: ~{int(len(book.split()) * 4/3)}\")   # rule of thumb: 100tokens=75words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hmtD77wMXdF"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9NUCaexPqy"
      },
      "source": [
        "To understand the token usage, you can check `usage_metadata`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LAoNQ3Ys-CB"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jzrjfNDxUhZ"
      },
      "source": [
        "You can also use `count_tokens` to check the size of your input prompt(s):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIrVpB-Htc3y"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE7MEKBI18K0"
      },
      "source": [
        "## Exercise: Chat with a book\n",
        "\n",
        "Task:\n",
        "- Create a chat\n",
        "- Use a system prompt: `\"You are an expert book reviewer with a witty tone.\"`\n",
        "- Use a temperature of `1.5`\n",
        "- Ask 1 to summarize the book\n",
        "- Ask 1 question to explain more detail about a certain topic from the book\n",
        "- Ask to create a social media post based on the book\n",
        "- Print the total number of tokens used during the chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sKL0JNbCzY0P"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muzBsZi5Fmgs"
      },
      "source": [
        "## Recap & Next steps\n",
        "\n",
        "You learned\n",
        "- Python SDK quickstart\n",
        "- Text prompting\n",
        "- Streaming and chats\n",
        "- System prompts and config options\n",
        "- Long context and token counting\n",
        "\n",
        "\n",
        "More helpful resources:\n",
        "- [API docs quickstart](https://ai.google.dev/gemini-api/docs/quickstart?lang=python)\n",
        "- [Text generation docs](https://ai.google.dev/gemini-api/docs/text-generation)\n",
        "- [Long context docs](https://ai.google.dev/gemini-api/docs/long-context)\n",
        "\n",
        "Next steps:\n",
        "- [Part 2: Multimodal understanding (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/cookbooks/part-2-multimodal-understanding.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
