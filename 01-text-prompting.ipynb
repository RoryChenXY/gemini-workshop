{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSc7AU66mJSC"
      },
      "source": [
        "##### Copyright 2025 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tc6tjo9vmJSE"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuC_VSKMcEt6"
      },
      "source": [
        "# Workshop: Build with Gemini (Part 1)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickloeber/workshop-build-with-gemini/blob/main/01-text-prompting.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This workshop teaches how to build with Gemini using the Gemini API and Python SDK.\n",
        "\n",
        "Course outline:\n",
        "\n",
        "- **Part 1 (this notebook): Quickstart + Text prompting**\n",
        "  - Text generation\n",
        "  - Token counting\n",
        "  - Streaming response\n",
        "  - Chats\n",
        "  - System prompts\n",
        "  - Configuration parameters\n",
        "  - Long context\n",
        "  - Final excercise: Chat with book\n",
        "\n",
        "- **[Part 2: Multimodal capabilities (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/02-multimodal-capabilities.ipynb)**\n",
        "\n",
        "- **[Part 3: Thinking models + agentic capabilities (tool usage)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/03-thinking-and-tools.ipynb)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avRVsnMMJvof"
      },
      "source": [
        "## 0. Use the Google AI Studio as playground\n",
        "\n",
        "Explore and play with all models in the [Google AI Studio](https://aistudio.google.com/apikey).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnl6q8tMcpwU"
      },
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKjUEGGzdp87"
      },
      "source": [
        "Install the [Google Gen AI Python SDK](https://github.com/googleapis/python-genai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4d9NjqNeAXx"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD1kaBP4dnZG"
      },
      "source": [
        "Get a free API key in the [Google AI Studio](https://aistudio.google.com/apikey).\n",
        "\n",
        "Configure the API key, the client, and define a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6raUs82eYfk"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except ImportError:\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# MODEL = \"gemini-2.0-flash\"\n",
        "# MODEL = \"gemini-2.5-pro\"\n",
        "# MODEL = \"gemini-2.5-flash-lite\"\n",
        "MODEL = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU8lsgD6nG-N"
      },
      "source": [
        " See all [models](https://ai.google.dev/gemini-api/docs/models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLsGbeGec8iF"
      },
      "source": [
        "## 2. Send your first prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57RFdZ6dRro"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Create 3 names for a vegan restaurant\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ChPLvinG-N"
      },
      "source": [
        "## 3. Token counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDWIKGMpnG-N"
      },
      "source": [
        "Count tokens before generation.\n",
        "\n",
        "Note that the latest pricing can be obtained from https://ai.google.dev/gemini-api/docs/pricing. The numbers below are approximations and may be out of date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yAUbtyOnG-N"
      },
      "outputs": [],
      "source": [
        "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "print(f\"# characters {len(prompt)}\")\n",
        "print(f\"# words {len(prompt.split())}\")\n",
        "# Try a rule of thumb (4 chars / token). These are always estimates.\n",
        "print(f\"# tokens: ~{int(len(prompt) / 4)}\")\n",
        "\n",
        "# Count tokens in the input\n",
        "token_count = client.models.count_tokens(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "print(f\"Input tokens: {token_count.total_tokens}\")\n",
        "\n",
        "# Estimate cost (example pricing for 2.5 Flash - always check current rates)\n",
        "COST_PER_MILLION_INPUT_TOKENS_USD = 0.30\n",
        "estimated_cost = token_count.total_tokens * COST_PER_MILLION_INPUT_TOKENS_USD / 1_000_000\n",
        "print(f\"Estimated input cost: ${estimated_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G9SX_7-nG-O"
      },
      "source": [
        "Count tokens after generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK-2FyY4nG-O"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "print()\n",
        "\n",
        "# Access token usage metadata\n",
        "usage = response.usage_metadata\n",
        "print(f\"Input tokens: {usage.prompt_token_count}\")\n",
        "print(f\"Thought tokens: {usage.thoughts_token_count}\")\n",
        "print(f\"Output tokens: {usage.candidates_token_count}\")\n",
        "print(f\"Total tokens: {usage.total_token_count}\")\n",
        "\n",
        "# Calculate total estimated cost\n",
        "COST_PER_MILLION_OUTPUT_TOKENS_USD = 2.50\n",
        "thought_tokens = int(usage.thoughts_token_count)\n",
        "total_cost = (usage.prompt_token_count * COST_PER_MILLION_INPUT_TOKENS_USD + (usage.candidates_token_count + thought_tokens) * COST_PER_MILLION_OUTPUT_TOKENS_USD) / 1_000_000\n",
        "print(f\"Total estimated cost: ${total_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqnTYJFdSlG"
      },
      "source": [
        "## 4. Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHRVaK0-tCE_"
      },
      "source": [
        "The simplest way to generate text is to provide the model with a text-only prompt. `contents` can be a single prompt, a list of prompt parts, or a combination of multimodal inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_HqjSiFsUQ2"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    #contents=\"Create 3 names for a vegan restaurant\",\n",
        "    #contents=[\"Create 3 names for a vegan restaurant\"],\n",
        "    contents=[\"Create 3 names for a vegan restaurant\", \"city: Perth\"]\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itCzXz1BiG5g"
      },
      "source": [
        "#### Streaming response\n",
        "\n",
        "By default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return the output as it is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d6HzwfZdWbt"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content_stream(\n",
        "    model=MODEL,\n",
        "    contents=[\"Explain how AI works\"]\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZjfCkzSdcEc"
      },
      "source": [
        "#### Chat\n",
        "\n",
        "The SDK [`Chat` class](https://googleapis.github.io/python-genai/genai.html#genai.chats.Chat) provides an interface to keep track of conversation history. Behind the scenes it uses the same [`generate_content`](https://googleapis.github.io/python-genai/genai.html#genai.models.Models.generate_content) method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCI8O9Ldjn6q"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL)\n",
        "\n",
        "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmfMuI44Kev2"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"I have 2 poodles\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_MkOG6uLs75"
      },
      "source": [
        "## 5. Configuration parameters\n",
        "\n",
        "Every prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_jk93Z-Lum-"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[\"Explain how AI works\"],\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=1024,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,\n",
        "        stop_sequences=None,\n",
        "        thinking_config=types.ThinkingConfig(\n",
        "          include_thoughts=True,\n",
        "          thinking_budget=100,\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPyrJ9ul7yuv"
      },
      "source": [
        "- `max_output_tokens`: Provides a mechanism for a maximum output length (including thought tokens). Can be helpful for avoiding costs in error scenarios when the expected answer is short.\n",
        "- `temperature`: [0, 2]. Controls randomness in token selection. Use <0.4 for more reproducibility, >0.7 for more diversity when re-run.\n",
        "- `top_p`: [0, 1]. Controls diversity. Lower values = more focused, higher = more diverse\n",
        "- `stop_sequences`: List of strings (up to 5) that tells the model to stop generating text if one of the strings is encountered in the response.\n",
        "- `thinking_config.include_thoughts`: Specify whether or not model thoughts should be generated as part of the response. Note that not all models support enabling or disabling thinking.\n",
        "- `thinking_config.thinking_budget`: How many tokens to budget for thoughts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9JgfKF8nvr"
      },
      "source": [
        "#### System instructions\n",
        "\n",
        "System instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CayVOonC8st5"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    config=types.GenerateContentConfig(system_instruction=\"You are Dumbledore. Be sure to welcome any new students.\"),\n",
        "    contents=\"Hello there\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjdRzLbN-ANo"
      },
      "source": [
        "## 6. Long context\n",
        "\n",
        "Gemini 2.0 and 2.5 models have a 1M token context window.\n",
        "\n",
        "In practice, 1 million tokens could look like:\n",
        "\n",
        "- 50,000 lines of code (with the standard 80 characters per line)\n",
        "- All the text messages you have sent in the last 5 years\n",
        "- 8 average length English novels\n",
        "- 1 hour of video data\n",
        "- ... or some combination of the above.\n",
        "\n",
        "For this step, you will feed in an entire book and ask questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6pGhOkj-CFS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "res = requests.get(\"https://gutenberg.org/cache/epub/16317/pg16317.txt\")\n",
        "book = res.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0nnKaKC-NMu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ï»¿The Project Gutenberg eBook of The Art of Public Speaking\r\n",
            "    \r\n",
            "This ebook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no res\n"
          ]
        }
      ],
      "source": [
        "print(book[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ves9N2m-_k-V"
      },
      "outputs": [],
      "source": [
        "print(f\"# characters {len(book)}\")\n",
        "print(f\"# words {len(book.split())}\")\n",
        "print(f\"# tokens: ~{int(len(book) / 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvtNgCnJzyXi"
      },
      "source": [
        "Since this is a longer prompt than before, calculate the token length accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kFvhexIzxMx"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"Summarize the book.\n",
        "\n",
        "Book:\n",
        "{book}\n",
        "\"\"\"\n",
        "\n",
        "token_response = client.models.count_tokens(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "print(token_response.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y3Ry4qA0J47"
      },
      "source": [
        "Now execute the prompt requesting a book summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hmtD77wMXdF"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE7MEKBI18K0"
      },
      "source": [
        "## !! Exercise: Chat with a book !!\n",
        "\n",
        "Create an interactive chat session where you can \"talk\" to the book \"Alice in Wonderland\". You'll set up the chat with a specific persona for the AI and use the book's text as context for the conversation.\n",
        "\n",
        "Tasks:\n",
        "- Download the text of \"Alice in Wonderland\" (helper code block is provided).\n",
        "- Create a chat session using `client.chats.create()`.\n",
        "- Use a system prompt: `\"You are an expert book reviewer with a witty tone.\"`\n",
        "- Use a temperature of `1.2`\n",
        "- Send an initial message to the chat session using `chat.send_message()`.\n",
        "- Send at least one follow-up question to the chat session and print its response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS-8Gy4hnG-P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# tokens: ~44365\n"
          ]
        }
      ],
      "source": [
        "res = requests.get(\"https://gutenberg.org/cache/epub/28885/pg28885.txt\")\n",
        "book = res.text\n",
        "print(f\"# tokens: ~{int(len(book) / 4)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKL0JNbCzY0P"
      },
      "outputs": [],
      "source": [
        "# TODO(you!): Create a chat and ask questions about the book"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muzBsZi5Fmgs"
      },
      "source": [
        "## Recap & Next steps\n",
        "\n",
        "Nice work! You learned:\n",
        "- The `google.genai` Python SDK\n",
        "- Text prompting\n",
        "- Token counting\n",
        "- Streaming and chats\n",
        "- System prompts and config options\n",
        "- Long context\n",
        "\n",
        "Key Takeaways:\n",
        "- Monitor token usage to control costs and stay within limits\n",
        "- Use streaming for interactive applications and long responses\n",
        "- Configure parameters based on your use case (factual vs creative content)\n",
        "- System instructions are powerful for setting behavior and tone\n",
        "\n",
        "More helpful resources:\n",
        "- [Text Generation Guide](https://ai.google.dev/gemini-api/docs/text-generation)\n",
        "- [Token Counting Guide](https://ai.google.dev/gemini-api/docs/tokens)\n",
        "- [Long Context Documentation](https://ai.google.dev/gemini-api/docs/long-context)\n",
        "\n",
        "Next steps:\n",
        "- [Part 2: Multimodal capabilities (image, video, audio, docs, code)](./02-multimodal-capabilities.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "01-text-prompting.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
