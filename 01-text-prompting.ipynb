{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSc7AU66mJSC"
      },
      "source": [
        "##### Copyright 2025 Patrick Loeber, Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tc6tjo9vmJSE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuC_VSKMcEt6"
      },
      "source": [
        "# Workshop: Build with Gemini (Part 1)\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickloeber/workshop-build-with-gemini/blob/main/01-text-prompting.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This workshop teaches how to build with Gemini using the Gemini API and Python SDK.\n",
        "\n",
        "Course outline:\n",
        "\n",
        "- **Part 1 (this notebook): Quickstart + Text prompting**\n",
        "  - Text generation\n",
        "  - Token counting\n",
        "  - Streaming response\n",
        "  - Chats\n",
        "  - System prompts\n",
        "  - Configuration parameters\n",
        "  - Long context\n",
        "  - Final excercise: Chat with book\n",
        "\n",
        "- **[Part 2: Multimodal capabilities (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/02-multimodal-capabilities.ipynb)**\n",
        "\n",
        "- **[Part 3: Thinking models + agentic capabilities (tool usage)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/03-thinking-and-tools.ipynb)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avRVsnMMJvof"
      },
      "source": [
        "## 0. Use the Google AI Studio as playground\n",
        "\n",
        "Explore and play with all models in the [Google AI Studio](https://aistudio.google.com/apikey).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnl6q8tMcpwU"
      },
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKjUEGGzdp87"
      },
      "source": [
        "Install the [Google Gen AI Python SDK](https://github.com/googleapis/python-genai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4d9NjqNeAXx",
        "outputId": "0970aed7-9f27-4146-aa6d-13e1878d1899"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD1kaBP4dnZG"
      },
      "source": [
        "Get a free API key in the [Google AI Studio](https://aistudio.google.com/apikey).\n",
        "\n",
        "Configure the API key, the client, and define a model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6raUs82eYfk"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "else:\n",
        "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# MODEL = \"gemini-2.5-pro-preview-06-05\"  # paid tier\n",
        "# MODEL = \"gemini-2.5-flash-preview-05-20\"\n",
        "# MODEL = \"gemini-2.0-flash-lite\"\n",
        "MODEL = \"gemini-2.0-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " See all [models](https://ai.google.dev/gemini-api/docs/models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLsGbeGec8iF"
      },
      "source": [
        "## 2. Send your first prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e57RFdZ6dRro",
        "outputId": "c0987493-7979-48d7-8275-47bd55f7f819"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=\"Create 3 names for a vegan restaurant\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Token counting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count tokens before generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "print(f\"# charakters {len(prompt)}\")\n",
        "print(f\"# words {len(prompt.split())}\")\n",
        "print(f\"# tokens: ~{int(len(prompt.split()) * 4/3)}\")   # rule of thumb: 100tokens=75words\n",
        "\n",
        "# Count tokens in the input\n",
        "token_count = client.models.count_tokens(\n",
        "    model=MODEL, \n",
        "    contents=prompt\n",
        ")\n",
        "print(f\"Input tokens: {token_count.total_tokens}\")\n",
        "\n",
        "# Estimate cost (example pricing for 2.0 Flash - check current rates)\n",
        "estimated_cost = token_count.total_tokens * 0.10 / 1_000_000\n",
        "print(f\"Estimated input cost: ${estimated_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count tokens after generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "# Access token usage metadata\n",
        "usage = response.usage_metadata\n",
        "print(f\"Input tokens: {usage.prompt_token_count}\")\n",
        "print(f\"Thought tokens: {usage.thoughts_token_count}\")\n",
        "print(f\"Output tokens: {usage.candidates_token_count}\")\n",
        "print(f\"Total tokens: {usage.total_token_count}\")\n",
        "\n",
        "# Calculate total estimated cost\n",
        "thought_tokens = usage.thoughts_token_count if usage.thoughts_token_count else 0\n",
        "total_cost = (usage.prompt_token_count * 0.10 + (usage.candidates_token_count + thought_tokens) * 0.4) / 1_000_000\n",
        "# total_cost = (usage.prompt_token_count * 0.15 + (usage.candidates_token_count + thought_tokens) * 3.5) / 1_000_000\n",
        "print(f\"Total estimated cost: ${total_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHqnTYJFdSlG"
      },
      "source": [
        "## 4. Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHRVaK0-tCE_"
      },
      "source": [
        "The simplest way to generate text is to provide the model with a text-only prompt. `contents` can be a single prompt, a list of prompts, or a combination of multimodal inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_HqjSiFsUQ2",
        "outputId": "d17984d4-46ad-4af4-94e1-b326481771d4"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    #contents=\"Create 3 names for a vegan restaurant\",\n",
        "    #contents=[\"Create 3 names for a vegan restaurant\"],\n",
        "    contents=[\"Create 3 names for a vegan restaurant\", \"city: Berlin\"]\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itCzXz1BiG5g"
      },
      "source": [
        "#### Streaming response\n",
        "\n",
        "By default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return outputs as they're generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d6HzwfZdWbt",
        "outputId": "e4ea9112-a647-4a4a-a13d-4121b0d38f29"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content_stream(\n",
        "    model=MODEL,\n",
        "    contents=[\"Explain how AI works\"]\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    print(chunk.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZjfCkzSdcEc"
      },
      "source": [
        "#### Chat\n",
        "\n",
        "The SDK chat class provides an interface to keep track of conversation history. Behind the scenes it uses the same `generate_content` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCI8O9Ldjn6q",
        "outputId": "ff023acd-e7db-4cdf-b4e8-e24271525f9b"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL)\n",
        "\n",
        "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmfMuI44Kev2",
        "outputId": "3125acf8-1483-42f3-ee19-1f855adb83dd"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"I have 2 poodles\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_MkOG6uLs75"
      },
      "source": [
        "## 5. Configuration parameters\n",
        "\n",
        "Every prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_jk93Z-Lum-",
        "outputId": "db565ae1-941b-4492-c560-92fd9e79a60c"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[\"Explain how AI works\"],\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=1024,\n",
        "        temperature=1.0,\n",
        "        top_p=0.95,  # Nucleus sampling - diversity of token selection\n",
        "        top_k=40,    # Consider top 40 most likely tokens\n",
        "        stop_sequences=None,\n",
        "        seed=1234,\n",
        "    )\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPyrJ9ul7yuv"
      },
      "source": [
        "- `max_output_tokens`: Prevents overly long responses and controls costs\n",
        "- `temperature`: [0, 2]. Controls randomness. Use <0.4 for factual content, >0.7 for creative content\n",
        "- `top_p`: [0, 1]. Controls diversity. Lower values = more focused, higher = more diverse\n",
        "- `top_k`: Limits token choices. Lower = more focused, higher = more diverse\n",
        "- `stop_sequences`: List of strings (up to 5) that tells the model to stop generating text if one of the strings is encountered in the response.\n",
        "- `seed`: If specified, the model makes a best effort to provide the same response for repeated requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9JgfKF8nvr"
      },
      "source": [
        "#### System instructions\n",
        "\n",
        "System instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CayVOonC8st5",
        "outputId": "a96fb99c-b8f7-4e41-a82d-127b8a60279e"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    # config=types.GenerateContentConfig(system_instruction=\"You are a Dumbledore.\"),\n",
        "    config={\"system_instruction\": \"You are Dumbledore.\"},\n",
        "    contents=\"Hello there\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjdRzLbN-ANo"
      },
      "source": [
        "## 6. Long context\n",
        "\n",
        "Gemini 2.0 and 2.5 models have a 1M token context window.\n",
        "\n",
        "In practice, 1 million tokens could look like:\n",
        "\n",
        "- 50,000 lines of code (with the standard 80 characters per line)\n",
        "- All the text messages you have sent in the last 5 years\n",
        "- 8 average length English novels\n",
        "- 1 hour of video data\n",
        "\n",
        "Let's feed in an entire book and ask questions:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6pGhOkj-CFS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "res = requests.get(\"https://gutenberg.org/cache/epub/16317/pg16317.txt\")\n",
        "book = res.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0nnKaKC-NMu",
        "outputId": "bf3fb659-2b0c-4cbc-8c5e-f295f7fa7998"
      },
      "outputs": [],
      "source": [
        "print(book[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ves9N2m-_k-V",
        "outputId": "fd365b44-732b-40c5-b5c1-557424adac03"
      },
      "outputs": [],
      "source": [
        "print(f\"# charakters {len(book)}\")\n",
        "print(f\"# words {len(book.split())}\")\n",
        "print(f\"# tokens: ~{int(len(book.split()) * 4/3)}\")   # rule of thumb: 100tokens=75words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hmtD77wMXdF",
        "outputId": "2c654f1f-fd80-4c7a-9c67-5129f0343477"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"Summarize the book.\n",
        "\n",
        "Book:\n",
        "{book}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE7MEKBI18K0"
      },
      "source": [
        "## !! Exercise: Chat with a book !!\n",
        "\n",
        "Create an interactive chat session where you can \"talk\" to the book \"Alice in Wonderland\". You'll set up the chat with a specific persona for the AI and use the book's text as context for the conversation.\n",
        "\n",
        "Tasks: \n",
        "- Download the text of \"Alice in Wonderland\" (helper code block is provided).\n",
        "- Create a chat session using `client.chats.create()`:\n",
        "- Use a system prompt: `\"You are an expert book reviewer with a witty tone.\"`\n",
        "- Use a temperature of `1.2`\n",
        "- Send an initial message to the chat session using `chat.send_message()`:\n",
        "- Send at least one follow-up question to the chat session and print its response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = requests.get(\"https://gutenberg.org/cache/epub/28885/pg28885.txt\")\n",
        "book = res.text\n",
        "print(f\"# tokens: ~{int(len(book.split()) * 4/3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKL0JNbCzY0P",
        "outputId": "d50dc228-9a2f-4c6d-9472-3ce381c783b5"
      },
      "outputs": [],
      "source": [
        "# TODO: create a chat and ask questions about the book"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muzBsZi5Fmgs"
      },
      "source": [
        "## Recap & Next steps\n",
        "\n",
        "Nice work! You learned:\n",
        "- Python SDK quickstart\n",
        "- Text prompting\n",
        "- Token counting\n",
        "- Streaming and chats\n",
        "- System prompts and config options\n",
        "- Long context\n",
        "\n",
        "Key Takeaways:\n",
        "- Monitor token usage to control costs and stay within limits\n",
        "- Use streaming for interactive applications and long responses\n",
        "- Configure parameters based on your use case (factual vs creative content)\n",
        "- System instructions are powerful for setting behavior and tone\n",
        "\n",
        "More helpful resources:\n",
        "- [Text Generation Guide](https://ai.google.dev/gemini-api/docs/text-generation)\n",
        "- [Token Counting Guide](https://ai.google.dev/gemini-api/docs/tokens)\n",
        "- [Long Context Documentation](https://ai.google.dev/gemini-api/docs/long-context)\n",
        "\n",
        "Next steps:\n",
        "- [Part 2: Multimodal capabilities (image, video, audio, docs, code)](https://github.com/patrickloeber/workshop-build-with-gemini/blob/main/02-multimodal-capabilities.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_FXjhH-VRl9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
